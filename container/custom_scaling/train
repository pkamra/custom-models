#!/usr/bin/env python

import os
import sys
import joblib
import pandas as pd
import traceback

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Define SageMaker paths
prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')

# Define input channel
channel_name = 'training'
training_path = os.path.join(input_path, channel_name)

# Function to execute training
def train():
    print('Starting the training.')
    try:
        # Read CSV files from the training path
        input_files = [os.path.join(training_path, file) for file in os.listdir(training_path) if file.endswith(".csv")]
        if not input_files:
            raise ValueError(f'No CSV files found in {training_path}. Check your data input path and permissions.')

        # Concatenate all CSV files into a single DataFrame
        raw_data = [pd.read_csv(file) for file in input_files]
        train_data = pd.concat(raw_data, ignore_index=True)

        # Check if the DataFrame is empty
        if train_data.empty:
            raise ValueError('Training data is empty. Ensure data is correctly uploaded.')

        # One-hot encode the 'Employment Status' feature
        employment_status_encoded = pd.get_dummies(train_data['Employment Status'])
        train_data = pd.concat([train_data, employment_status_encoded], axis=1)
        train_data[employment_status_encoded.columns] = train_data[employment_status_encoded.columns].astype(int)
        train_data.drop('Employment Status', axis=1, inplace=True)

        # Define the features and target variable
        features_to_standardize = ['Age', 'Annual Income', 'Credit Score', 
                                   'Years at Current Residence', 'Number of Defaults', 'Loan Amount'] + list(employment_status_encoded.columns)
        X = train_data[features_to_standardize]
        y = train_data['Risk Category']

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Initialize the StandardScaler and fit it on the training data
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        
        # Convert scaled training data back to DataFrame for easy inspection
        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features_to_standardize)
        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features_to_standardize)

        # Print the first few rows of the scaled DataFrame
        print("Scaled Training Data:")
        print(X_train_scaled_df.head())

        print("\nScaled Testing Data:")
        print(X_test_scaled_df.head())
        

        # Save the scaler
        scaler_filename = os.path.join(model_path, 'scaler.joblib')
        joblib.dump(scaler, scaler_filename)

        print(f"Scaler saved as: {scaler_filename}")

    except Exception as e:
        # Write an error file for failure description
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)

        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        sys.exit(255)

if __name__ == '__main__':
    train()
    sys.exit(0)  # Indicate success
